# OfflineAI ü§ñ

> An offline-first AI-powered mobile app using small language models (SLMs)

[![React Native](https://img.shields.io/badge/React%20Native-0.83.1-blue.svg)](https://reactnative.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.8.3-blue.svg)](https://www.typescriptlang.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## üì± Overview

**OfflineAI** is a React Native mobile application that brings the power of AI language models directly to your device. With a focus on privacy and offline functionality, this app runs small language models (SLMs) locally on your phone, ensuring your conversations stay private and work without an internet connection.

### ‚ú® Key Features

- üîí **Privacy-First**: All AI processing happens on-device - your data never leaves your phone
- üìµ **Fully Offline**: No internet connection required after initial setup
- üöÄ **Fast & Responsive**: Optimized for mobile performance with efficient model inference
- üí¨ **Conversational Context**: Maintains conversation history for coherent multi-turn dialogues
- üé® **Beautiful UI**: Modern, intuitive interface with markdown support for rich message formatting
- üì¶ **Model Management**: Easy model selection and loading from local storage
- üíæ **Persistent Storage**: Conversations and settings saved locally using MMKV

## üèóÔ∏è Architecture

### Tech Stack

- **Framework**: React Native 0.83.1
- **Language**: TypeScript 5.8.3
- **State Management**: Redux Toolkit
- **AI Engine**: [llama.rn](https://github.com/mybigday/llama.rn) - On-device LLM inference
- **Storage**: 
  - MMKV for fast key-value storage
  - SQLite for structured data
- **UI Components**:
  - React Native Markdown Display for rich text rendering
  - Custom components for chat interface

### Project Structure

```
OfflineAI/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/       # Reusable UI components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChatInput.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MessageBubble.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ModelSelector.tsx
‚îÇ   ‚îú‚îÄ‚îÄ screens/          # App screens
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ChatScreen.tsx
‚îÇ   ‚îú‚îÄ‚îÄ store/            # Redux state management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chatSlice.ts
‚îÇ   ‚îú‚îÄ‚îÄ hooks/            # Custom React hooks
‚îÇ   ‚îú‚îÄ‚îÄ services/         # Business logic & API services
‚îÇ   ‚îú‚îÄ‚îÄ db/               # Database & storage utilities
‚îÇ   ‚îú‚îÄ‚îÄ utils/            # Helper functions & utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ promptTemplates.ts
‚îÇ   ‚îî‚îÄ‚îÄ constants/        # App constants
‚îú‚îÄ‚îÄ models/               # Local AI models storage
‚îú‚îÄ‚îÄ ios/                  # iOS native code
‚îú‚îÄ‚îÄ android/              # Android native code
‚îî‚îÄ‚îÄ App.tsx              # App entry point
```

### Prompt Templates

The app supports multiple prompt formats for different model types:

- **ChatML**: Default format for Llama 3 and TinyLlama models
- **Alpaca**: Alternative format for Alpaca-based models

## üöÄ Getting Started

### Prerequisites

Before you begin, ensure you have:

- **Node.js** >= 20
- **React Native development environment** set up ([Setup Guide](https://reactnative.dev/docs/set-up-your-environment))
- **Xcode** (for iOS development)
- **Android Studio** (for Android development)

### Installation

1. **Clone the repository**

```bash
git clone https://github.com/dbadea-heits/OfflineAI.git
cd OfflineAI
```

2. **Install dependencies**

```bash
npm install
```

3. **Install iOS dependencies** (iOS only)

```bash
# Install Ruby bundler (first time only)
bundle install

# Install CocoaPods dependencies
cd ios
bundle exec pod install
cd ..
```

4. **Add AI models**

Place your GGUF model files in the `models/` directory. Compatible models include:
- TinyLlama
- Llama 3
- Phi-2
- Other GGUF format models

### Running the App

#### Start Metro Bundler

```bash
npm start
```

#### Run on iOS

```bash
npm run ios
```

#### Run on Android

```bash
npm run android
```

## üìñ Usage

1. **Select a Model**: On first launch, use the model selector to choose an AI model
2. **Start Chatting**: Type your message in the input field and press send
3. **View Responses**: AI responses are rendered with markdown support
4. **Continue Conversations**: The app maintains context across multiple messages

## üõ†Ô∏è Development

### Available Scripts

- `npm start` - Start Metro bundler
- `npm run ios` - Run on iOS simulator
- `npm run android` - Run on Android emulator
- `npm test` - Run tests
- `npm run lint` - Lint code with ESLint

### Code Style

This project uses:
- **ESLint** for code linting
- **Prettier** for code formatting
- **TypeScript** for type safety

## üß™ Testing

```bash
npm test
```

## üì¶ Building for Production

### iOS

```bash
# Build for iOS
cd ios
xcodebuild -workspace OfflineAI.xcworkspace -scheme OfflineAI -configuration Release
```

### Android

```bash
# Build APK
cd android
./gradlew assembleRelease
```

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- [llama.rn](https://github.com/mybigday/llama.rn) - For making on-device LLM inference possible
- [React Native](https://reactnative.dev/) - For the amazing mobile framework
- The open-source AI community for developing small, efficient language models

## üìß Contact

Dan Badea - [@dbadea-heits](https://github.com/dbadea-heits)

Project Link: [https://github.com/dbadea-heits/OfflineAI](https://github.com/dbadea-heits/OfflineAI)

---

**Built with ‚ù§Ô∏è for privacy-conscious AI enthusiasts**
